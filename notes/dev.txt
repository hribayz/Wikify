User Selects wiki page - URL upload
Validate page
Download page as it is
Identify media on page
Determine legal availability of a piece of media
Generate attributions
Link to attributions somewhere somehow
Display attributions / each product has own page
Display warning when legal stuff uncertain (living person...)
Create image of page
User can switch between sizes, details (advanced)
User selects product, image displayed real time
User can proceed to eshop

Nothing says x like y generator
Wiki content Buffer, Wiki Traffic controller
Image generator
Sold products images, license database
Eshop, orders, clients database
License generator
Eshop

How to get wikipedia content

	option 1 : use revisions api. Outputs wikitext. Good for content rather than the full page as it is.
	option 2 : use parse api. https://en.wikipedia.org/w/api.php?action=parse
	option 3 : use textExtract api. Limited to 1200 chars.
	option 4 : query wikipedia directly. Same as parse api but has all outer html as well.
	
The api can return _some_ interesting outputs (like list of image urls) but most will have to be parsed from the raw html anyway. It's a parsed wikitext with some extracted information, not a complete composition of prebuilt blocks.

Server/client image preview/full generation

	option 1: client-side configurator (better if there's too many options)
		start with this, do a MVP
	option 2: server-side generated image for given options (OK if it's just article + product combination)
		if option 1 no longer sufficient for the job
		
	resolution: probably eventually both but starting with option 2 as 1st iteration MVP is worth the extra work

Image composition

	display html in a browser window, then screenshot?
	(glue bitmap elements together?)

Elements handling
	
	break original page into elements, compose new page (reusing some of them from scratch) from element prototypes, send to browser.
	cut known unwanted elements from page, send the rest to browser as is.
	
	who will cut the page into elements?
	how to store the elements structure? keep the original html or try to come up with something new? Answer: come up with smth new.
		1) There is too much extra information and decision making related to page elements. Additional data structure next to the original html that will hold that all is better than injecting into html or deriving from ad hoc. Is it image of a living person? What is the license?
		2) Editing an html and then displaying it sounds like a recipe for disaster. Fragile, reactionary in case smth changes...
		3) Knowing all elements will enable building element picker.
	
IComponent<T>

	Contains information extrinsic to the media itself.
	Contains everything that can change in time, with use case
	what is its license?
	who says if we can use it for display?
	what is its MIME type? Photo, map, enumeration, quote, text block, header... do we need to know?
	what is its IRL type? living person, map, 
	
AWikiArticle

	Smart container or not?
	abstract class with baked in data containing members and abstract/virtual behaviour
	or
	just (abstract) container class and extract behaviour out. (visitor pattern)

How to decide what elements are valuable?
	
	Keep a populated data structure with known elements. Downloader will compare new ones against the database, keep known elements and throw away all other. Better than the other way around as there will be more useless elements than useful. Omission will result into missing some minor element which is better than accidentally including some rubbish.
	Build the data structure at startup and keep in memory. Keep data as file.
	
Build parameters

	{Include / collapse / do not include} {content panel, part of a series sidebar, infobox biography vcard, wikitable, hatnote (see also),...}
	Put all photos on one side
	Do not include photos of living persons.

Even the simplest container can have some logic

Should it even be possible to have a media container without license?

License
	
	Only include media with comercially enabled licenses
	License everything as CC-BY-SA 4.0
	Do not have to open source the code that produced the derivative work
	probably can include photos of living persons: https://en.wikipedia.org/wiki/Model_release
	
WikiContentModel does not reflect all types of media -- only relevant to article
wikipedia is quirky
https://www.mediawiki.org/wiki/Manual:Deciding_whether_to_use_a_wiki_as_your_website_type

Who knows about media or article whereabouts: ID provider or Archive. ID should be independent on archive implementation, archive should target its own resource. 	
	